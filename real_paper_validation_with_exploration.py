#!/usr/bin/env python3
"""
Real Paper Validation with Forced Exploration

This script runs the real agent validation with forced exploration enabled
to test the impact of epsilon-greedy exploration on performance.
"""

import asyncio
import json
import logging
import os
import time
from datetime import datetime
from typing import Dict, List, Any, Set
import openai
import anthropic

# Import original paper components
from src.agents.agent_types import ClientAgent, ContractorAgent, Task, TaskType, HardwareConfiguration, NetworkConfiguration
from src.decision.decision_engine import DecisionEngine
from src.cost_model.cost_calculator import CostCalculator

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("RealPaperValidationWithExploration")


class RealLLMContractor(ContractorAgent):
    """Real LLM contractor that makes actual API calls."""
    
    def __init__(self, name: str, api_key: str, model_type: str, **kwargs):
        super().__init__(name=name, **kwargs)
        self.api_key = api_key
        self.model_type = model_type  # 'openai' or 'anthropic'
        self.demo_mode = api_key.startswith("demo-key")
        
        # Initialize API clients
        if not self.demo_mode:
            if model_type == 'openai':
                self.client = openai.OpenAI(api_key=api_key)
            elif model_type == 'anthropic':
                self.client = anthropic.Anthropic(api_key=api_key)
        else:
            self.client = None
            
        # Set skills based on model type
        if model_type == 'openai':
            self.skills = {"financial_rag", "sentiment_analysis", "risk_assessment"}
        elif model_type == 'anthropic':
            self.skills = {"financial_rag", "portfolio_optimization", "risk_assessment"}
        else:
            self.skills = {"financial_rag"}
            
    async def execute_real_task(self, task: Task) -> Dict[str, Any]:
        """Execute task using real LLM API."""
        start_time = time.time()
        
        try:
            if self.model_type == 'openai' and not self.demo_mode:
                result = await self._execute_openai_task(task)
            elif self.model_type == 'anthropic' and not self.demo_mode:
                result = await self._execute_anthropic_task(task)
            else:
                result = await self._execute_demo_task(task)
                
            execution_time = time.time() - start_time
            
            return {
                'success': True,
                'result': result,
                'execution_time': execution_time,
                'actual_cost': result.get('cost', self.base_price_per_task),
                'agent': self.name,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"{self.name} task failed: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'execution_time': execution_time,
                'actual_cost': 0.0,
                'agent': self.name,
                'timestamp': datetime.now().isoformat()
            }
    
    async def _execute_openai_task(self, task: Task) -> Dict[str, Any]:
        """Execute task using OpenAI API."""
        prompt = self._generate_task_prompt(task)
        
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a financial AI assistant providing detailed analysis."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        # Calculate actual cost based on token usage
        cost = (response.usage.prompt_tokens / 1000) * 0.03 + (response.usage.completion_tokens / 1000) * 0.06
        
        return {
            'analysis': response.choices[0].message.content,
            'model': 'gpt-4',
            'cost': cost,
            'tokens': response.usage.total_tokens
        }
    
    async def _execute_anthropic_task(self, task: Task) -> Dict[str, Any]:
        """Execute task using Anthropic API."""
        prompt = self._generate_task_prompt(task)
        
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            temperature=0.3,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        # Calculate actual cost based on token usage
        cost = (response.usage.input_tokens / 1000) * 0.015 + (response.usage.output_tokens / 1000) * 0.075
        
        return {
            'analysis': response.content[0].text,
            'model': 'claude-3-5-sonnet',
            'cost': cost,
            'tokens': response.usage.input_tokens + response.usage.output_tokens
        }
    
    async def _execute_demo_task(self, task: Task) -> Dict[str, Any]:
        """Execute task in demo mode (simulated)."""
        await asyncio.sleep(0.1)  # Simulate processing time
        
        return {
            'analysis': f"Demo analysis for {task.task_type} task {task.task_id}",
            'model': f'{self.model_type}-demo',
            'cost': self.base_price_per_task,
            'tokens': 500
        }
    
    def _generate_task_prompt(self, task: Task) -> str:
        """Generate appropriate prompt based on task type."""
        prompts = {
            "financial_rag": f"Analyze financial documents with {task.document_count} documents totaling {task.data_size_gb}GB. Provide key insights and risk assessment.",
            "sentiment_analysis": f"Perform sentiment analysis on financial data with complexity factor {task.complexity_factor}. Provide sentiment scores and market implications.",
            "risk_assessment": f"Conduct comprehensive risk assessment for financial portfolio with criticality level {task.criticality}. Identify key risks and mitigation strategies.",
            "portfolio_optimization": f"Optimize investment portfolio with budget constraint ${task.budget_constraint}. Provide allocation recommendations and expected returns."
        }
        return prompts.get(task.task_type, f"Analyze task of type {task.task_type}")


class RealLocalAgent:
    """Real local agent that performs actual computation."""
    
    def __init__(self, client: ClientAgent):
        self.client = client
        self.name = "Local-Real"
        
    async def execute_real_task(self, task: Task) -> Dict[str, Any]:
        """Execute task using real local computation."""
        start_time = time.time()
        
        try:
            # Simulate real computation based on task complexity
            computation_time = task.complexity_factor * task.estimated_duration_hours * 0.1  # Scale down for demo
            await asyncio.sleep(min(computation_time, 1.0))  # Cap at 1 second for demo
            
            # Perform actual computation
            result = self._perform_computation(task)
            
            execution_time = time.time() - start_time
            cost = self.client.hardware.compute_cost_per_hour * (execution_time / 3600)
            
            return {
                'success': True,
                'result': result,
                'execution_time': execution_time,
                'actual_cost': cost,
                'agent': self.name,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Local task failed: {e}")
            
            return {
                'success': False,
                'error': str(e),
                'execution_time': execution_time,
                'actual_cost': 0.0,
                'agent': self.name,
                'timestamp': datetime.now().isoformat()
            }
    
    def _perform_computation(self, task: Task) -> Dict[str, Any]:
        """Perform actual local computation."""
        # Simple computation based on task type
        if task.task_type == "financial_rag":
            # Simulate document processing
            processed_docs = task.document_count * task.complexity_factor
            return {
                'processed_documents': int(processed_docs),
                'insights_extracted': int(processed_docs * 0.1),
                'computation_method': 'local_cpu_gpu'
            }
        elif task.task_type == "portfolio_optimization":
            # Simulate optimization
            return {
                'optimal_allocation': {'stocks': 0.6, 'bonds': 0.3, 'cash': 0.1},
                'expected_return': 0.08 * task.complexity_factor,
                'risk_score': 0.15 / task.complexity_factor,
                'computation_method': 'local_optimization'
            }
        else:
            return {
                'analysis': f"Local analysis for {task.task_type}",
                'computation_method': 'local_processing'
            }


class RealPaperValidationWithExploration:
    """Validation using original paper implementation with real agents and forced exploration."""
    
    def __init__(self):
        self.decision_engine = DecisionEngine()
        # FORCE EXPLORATION FOR THIS TEST
        self.decision_engine.force_exploration = True
        self.cost_calculator = CostCalculator()
        self.results: List[Dict] = []
        
    def create_real_contractors(self) -> List[RealLLMContractor]:
        """Create real LLM contractors."""
        openai_key = os.getenv('OPENAI_API_KEY', 'demo-key-openai')
        anthropic_key = os.getenv('ANTHROPIC_API_KEY', 'demo-key-anthropic')
        
        contractors = [
            RealLLMContractor(
                name="GPT-4-Real",
                api_key=openai_key,
                model_type="openai",
                specialization="gpu_specialist",
                base_price_per_task=2.0,
                reliability_score=0.95,
                avg_latency_minutes=15.0,
                security_risk_score=0.05,
                hardware=HardwareConfiguration(
                    gpu_model="NVIDIA H100",
                    gpu_count=8,
                    gpu_memory_gb=640,
                    gpu_flops=1000e12,
                    cpu_model="AMD EPYC 9654",
                    cpu_cores=96,
                    cpu_memory_gb=768,
                    peak_flops=1000e12,
                    total_memory_gb=1408,
                    utilization_factor=0.85,
                    tdp_watts=700,
                    purchase_cost=150000,
                    compute_cost_per_hour=8.0,
                    memory_cost_per_hour=2.0
                )
            ),
            
            RealLLMContractor(
                name="Claude-3-Real",
                api_key=anthropic_key,
                model_type="anthropic",
                specialization="hybrid_cloud",
                base_price_per_task=1.5,
                reliability_score=0.92,
                avg_latency_minutes=18.0,
                security_risk_score=0.06,
                hardware=HardwareConfiguration(
                    gpu_model="NVIDIA A100",
                    gpu_count=4,
                    gpu_memory_gb=320,
                    gpu_flops=624e12,
                    cpu_model="Intel Xeon Platinum 8380",
                    cpu_cores=80,
                    cpu_memory_gb=512,
                    peak_flops=624e12,
                    total_memory_gb=832,
                    utilization_factor=0.80,
                    tdp_watts=500,
                    purchase_cost=80000,
                    compute_cost_per_hour=6.0,
                    memory_cost_per_hour=1.5
                )
            ),
            
            RealLLMContractor(
                name="Budget-Cloud-Real",
                api_key="demo-key-budget",
                model_type="budget",
                specialization="budget_provider",
                base_price_per_task=0.8,
                reliability_score=0.85,
                avg_latency_minutes=45.0,
                security_risk_score=0.12,
                hardware=HardwareConfiguration(
                    gpu_model="NVIDIA RTX 4090",
                    gpu_count=2,
                    gpu_memory_gb=48,
                    gpu_flops=165e12,
                    cpu_model="AMD Ryzen 9 7950X",
                    cpu_cores=32,
                    cpu_memory_gb=128,
                    peak_flops=165e12,
                    total_memory_gb=176,
                    utilization_factor=0.70,
                    tdp_watts=350,
                    purchase_cost=25000,
                    compute_cost_per_hour=3.0,
                    memory_cost_per_hour=0.8
                )
            )
        ]
        
        return contractors
    
    def create_real_client(self) -> ClientAgent:
        """Create real client agent."""
        return ClientAgent(
            name="Financial-Firm-Client",
            hardware=HardwareConfiguration(
                gpu_model="NVIDIA RTX 3080",
                gpu_count=1,
                gpu_memory_gb=10,
                gpu_flops=29.77e12,
                cpu_model="Intel Xeon E5-2690 v4",
                cpu_cores=14,
                cpu_memory_gb=64,
                peak_flops=29.77e12,
                total_memory_gb=74,
                utilization_factor=0.75,
                tdp_watts=320,
                purchase_cost=12000,
                compute_cost_per_hour=0.45,
                memory_cost_per_hour=0.15
            )
        )
    
    def create_real_tasks(self) -> List[Task]:
        """Create real tasks for validation."""
        tasks = []
        
        # Financial RAG tasks
        for i in range(15):
            tasks.append(Task(
                task_type="financial_rag",
                document_count=50000 + i * 5000,
                data_size_gb=8.2 + i * 0.5,
                complexity_factor=0.8 + i * 0.1,
                estimated_duration_hours=2.0 + i * 0.2,
                value=100.0 + i * 10,
                budget_constraint=50.0 + i * 5,
                required_skills=["financial_analysis", "document_processing"]
            ))
        
        # Risk assessment tasks
        for i in range(10):
            tasks.append(Task(
                task_type="risk_assessment",
                document_count=25000 + i * 2500,
                data_size_gb=4.1 + i * 0.3,
                complexity_factor=1.0 + i * 0.15,
                estimated_duration_hours=1.5 + i * 0.15,
                value=80.0 + i * 8,
                budget_constraint=40.0 + i * 4,
                required_skills=["risk_analysis", "statistical_modeling"]
            ))
        
        # Portfolio optimization tasks
        for i in range(10):
            tasks.append(Task(
                task_type="portfolio_optimization",
                document_count=10000 + i * 1000,
                data_size_gb=2.0 + i * 0.2,
                complexity_factor=1.2 + i * 0.1,
                estimated_duration_hours=3.0 + i * 0.3,
                value=150.0 + i * 15,
                budget_constraint=75.0 + i * 7.5,
                required_skills=["optimization", "portfolio_management"]
            ))
        
        return tasks
    
    async def run_validation(self) -> Dict[str, Any]:
        """Run the real paper validation with forced exploration."""
        print("🔬 REAL PAPER VALIDATION WITH FORCED EXPLORATION")
        print("=" * 60)
        print("Using original COALESCE implementation with real LLM agents")
        print("🎯 EXPLORATION FORCED: Every task will explore contractors")
        print("=" * 60)
        
        # Create agents and tasks
        client = self.create_real_client()
        contractors = self.create_real_contractors()
        local_agent = RealLocalAgent(client)
        tasks = self.create_real_tasks()
        
        print(f"\n📋 Created client with {len(contractors)} real contractors")
        print(f"📝 Created {len(tasks)} real tasks")
        print(f"🎯 Exploration rate: FORCED (100% exploration)")
        
        # Execute tasks using original COALESCE decision engine with forced exploration
        print("\n🔄 Executing tasks with FORCED exploration...")
        
        successful_decisions = 0
        total_cost_savings = 0.0
        total_time_savings = 0.0
        outsourcing_decisions = 0
        exploration_decisions = 0
        
        for i, task in enumerate(tasks, 1):
            if i % 10 == 0:
                print(f"Progress: {i}/{len(tasks)} tasks completed")
            
            try:
                # Use original COALESCE decision engine with forced exploration
                decision = self.decision_engine.make_decision(client, task, contractors)
                
                # Count exploration decisions
                if decision.exploration:
                    exploration_decisions += 1
                
                # Execute the task based on decision
                if decision.decision == 'OUTSOURCE' and decision.selected_contractor:
                    # Execute with selected contractor
                    contractor = decision.selected_contractor
                    if isinstance(contractor, RealLLMContractor):
                        result = await contractor.execute_real_task(task)
                    else:
                        # Fallback for non-real contractors
                        result = {
                            'success': True,
                            'actual_cost': decision.selected_contractor.base_price_per_task,
                            'execution_time': decision.selected_contractor.avg_latency_minutes * 60,
                            'agent': decision.selected_contractor.name
                        }
                    outsourcing_decisions += 1
                else:
                    # Execute locally
                    result = await local_agent.execute_real_task(task)
                
                if result['success']:
                    successful_decisions += 1
                    total_cost_savings += decision.cost_savings
                    total_time_savings += decision.time_savings
                    
                    # Store result with COALESCE analysis
                    self.results.append({
                        'task_id': task.task_id,
                        'task_type': task.task_type,
                        'decision': decision.decision,
                        'selected_agent': result['agent'],
                        'topsis_score': decision.topsis_score,
                        'confidence': decision.confidence,
                        'cost_savings': decision.cost_savings,
                        'time_savings': decision.time_savings,
                        'actual_cost': result['actual_cost'],
                        'execution_time': result['execution_time'],
                        'exploration': decision.exploration,
                        'criteria_scores': {
                            'cost': decision.criteria_scores.cost if decision.criteria_scores else 0,
                            'reliability': decision.criteria_scores.reliability if decision.criteria_scores else 0,
                            'latency': decision.criteria_scores.latency if decision.criteria_scores else 0,
                            'security': decision.criteria_scores.security if decision.criteria_scores else 0,
                            'skill_compatibility': decision.criteria_scores.skill_compatibility if decision.criteria_scores else 0
                        }
                    })
                    
                    exploration_marker = "🎯" if decision.exploration else "📊"
                    print(f"{exploration_marker} Task {i}: {result['agent']} - {decision.cost_savings:.1f}% savings (TOPSIS: {decision.topsis_score:.3f})")
                
            except Exception as e:
                print(f"❌ Task {i} failed: {e}")
        
        # Calculate final metrics
        success_rate = successful_decisions / len(tasks)
        avg_cost_savings = total_cost_savings / successful_decisions if successful_decisions > 0 else 0
        avg_time_savings = total_time_savings / successful_decisions if successful_decisions > 0 else 0
        outsourcing_rate = (outsourcing_decisions / len(tasks)) * 100
        exploration_rate = (exploration_decisions / len(tasks)) * 100
        
        # Calculate TOPSIS score average
        topsis_scores = [r['topsis_score'] for r in self.results if r['topsis_score'] > 0]
        avg_topsis_score = sum(topsis_scores) / len(topsis_scores) if topsis_scores else 0
        
        results = {
            'validation_type': 'REAL_PAPER_IMPLEMENTATION_WITH_EXPLORATION',
            'total_tasks': len(tasks),
            'successful_tasks': successful_decisions,
            'success_rate': success_rate,
            'avg_cost_savings_percent': avg_cost_savings,
            'avg_time_savings_percent': avg_time_savings,
            'outsourcing_rate_percent': outsourcing_rate,
            'exploration_rate_percent': exploration_rate,
            'avg_topsis_score': avg_topsis_score,
            'paper_comparison': {
                'paper_cost_reduction_mean': 43.5,
                'validation_cost_reduction': avg_cost_savings,
                'paper_time_savings_mean': 44.4,
                'validation_time_savings': avg_time_savings,
                'paper_outsourcing_rate': 33.8,
                'validation_outsourcing_rate': outsourcing_rate,
                'paper_topsis_mean': 0.79,
                'validation_topsis': avg_topsis_score
            },
            'detailed_results': self.results
        }
        
        # Print summary
        print(f"\n📊 VALIDATION SUMMARY (WITH FORCED EXPLORATION)")
        print(f"Total tasks: {len(tasks)}")
        print(f"Successful tasks: {successful_decisions}")
        print(f"Success rate: {success_rate:.1%}")
        print(f"Average cost savings: {avg_cost_savings:.1f}%")
        print(f"Average time savings: {avg_time_savings:.1f}%")
        print(f"Outsourcing rate: {outsourcing_rate:.1f}%")
        print(f"Exploration rate: {exploration_rate:.1f}%")
        print(f"Average TOPSIS score: {avg_topsis_score:.3f}")
        
        return results


async def main():
    """Main execution function."""
    validation = RealPaperValidationWithExploration()
    results = await validation.run_validation()
    
    # Save results with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"real_paper_validation_with_exploration_{timestamp}.json"
    
    with open(filename, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n💾 Results saved to {filename}")


if __name__ == "__main__":
    asyncio.run(main())